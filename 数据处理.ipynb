{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从文件中读取数据\n",
    "alpha_df_17=pd.read_csv(\"factor_data.csv\",dtype={\"stock_code\":str})\n",
    "alpha_df_16=pd.read_csv(\"factor_data_16.csv\",dtype={\"stock_code\":str})\n",
    "alpha_df_15=pd.read_csv(\"factor_data_15.csv\",dtype={\"stock_code\":str})\n",
    "alpha_df_14=pd.read_csv(\"factor_data_14.csv\",dtype={\"stock_code\":str})\n",
    "alpha_df_13=pd.read_csv(\"factor_data_13.csv\",dtype={\"stock_code\":str})\n",
    "alpha_df_12=pd.read_csv(\"factor_data_12.csv\",dtype={\"stock_code\":str})\n",
    "alpha_df=pd.concat([alpha_df_17,alpha_df_16,alpha_df_15,alpha_df_14,alpha_df_13,alpha_df_12])\n",
    "\n",
    "alpha_df.rename(columns={\"date\":\"datetime\",\"stock_code\":\"code\"},inplace=True)\n",
    "alpha_df[\"code\"]=alpha_df[\"code\"].apply(lambda x:x+\".SH\" if x[0]==\"6\" else x+\".SZ\")\n",
    "alpha_df.sort_values(by=[\"datetime\",\"code\"],ascending=[False,True],inplace=True)\n",
    "\n",
    "#添加上股票的行业信息\n",
    "data=pd.read_csv(\"data_needs.csv\")\n",
    "date=list(data[\"datetime\"].drop_duplicates())[-1]\n",
    "df=data.groupby(\"datetime\").get_group(date)\n",
    "dic=dict(zip(df[\"code\"],df[\"industry_code\"]))\n",
    "alpha_df[\"industry_code\"]=alpha_df[\"code\"].apply(lambda x:dic.get(x,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_df.set_index(\"code\",inplace=True)\n",
    "\n",
    "#去除st股票\n",
    "st_stock=list(pd.read_csv(\"st_stocks.csv\")[\"code\"])\n",
    "alpha_df.drop(st_stock,inplace=True)\n",
    "\n",
    "#去除新股和停牌股\n",
    "grouped=alpha_df.groupby(\"datetime\")\n",
    "date_list=list(alpha_df[\"datetime\"].drop_duplicates())\n",
    "code_list=list(alpha_df.index.drop_duplicates())\n",
    "join=set(code_list)\n",
    "union=set(code_list)\n",
    "for date in date_list:\n",
    "    df=grouped.get_group(date)\n",
    "    stock_set=set(df.index)\n",
    "    join=stock_set&join\n",
    "    union=stock_set|union\n",
    "other_list=list(union-join)\n",
    "alpha_df.drop(other_list,inplace=True)\n",
    "\n",
    "alpha_df.to_csv(\"zz_data.csv\",index_label=\"code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-757d95730e80>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0mfactor_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BETA'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'RSTR'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'LNCAP'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ETOP'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DASTD'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'EGRO'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'BTOP'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'DTOA'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'STOM'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0malpha_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"zz_data.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[0mdf_1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmedian_fill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfactor_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#行业中位数填充缺失值\n",
    "def median_fill(alpha_df,factor_list):\n",
    "    grouped = alpha_df.groupby(\"datetime\")\n",
    "    date_list = list(alpha_df[\"datetime\"].drop_duplicates())\n",
    "    filled_df = pd.DataFrame()\n",
    "\n",
    "    flag = 0\n",
    "\n",
    "    for date in date_list:\n",
    "        print(date,end=\" \")\n",
    "        panel_data = grouped.get_group(date)\n",
    "\n",
    "        df = pd.DataFrame(panel_data)\n",
    "\n",
    "        # 只留下需要处理的列\n",
    "        cols = factor_list\n",
    "        # 分组的列\n",
    "        gp_col = 'industry_code'\n",
    "        # 查询nan的列\n",
    "        df_na = df[cols].isnull()\n",
    "        # 根据分组计算中位数\n",
    "        df_median = df.groupby(gp_col)[cols].median()\n",
    "        # 依次处理每一列\n",
    "        for col in cols:\n",
    "            na_series = df_na[col]\n",
    "            names = list(df.loc[na_series, gp_col])\n",
    "\n",
    "            t = df_median.loc[names, col]\n",
    "            t.index = df.loc[na_series, col].index\n",
    "            t.fillna(t.mean(),inplace=True)\n",
    "\n",
    "\n",
    "            # 相同的index进行赋值\n",
    "            df.loc[na_series, col] = t\n",
    "\n",
    "        if flag == 0:\n",
    "            filled_df = df\n",
    "            flag = 1\n",
    "        else:\n",
    "            filled_df = pd.concat([filled_df, df])\n",
    "\n",
    "\n",
    "    #filled_df.to_csv(\"filled_data.csv\", index=False, float_format=\"%.6f\")\n",
    "    return filled_df\n",
    "\n",
    "\n",
    "factor_list=['BETA','RSTR','LNCAP','ETOP','DASTD','EGRO','BTOP','DTOA','STOM']\n",
    "alpha_df=pd.read_csv(\"zz_data.csv\")\n",
    "df_1=median_fill(alpha_df,factor_list)\n",
    "\n",
    "\n",
    "#中位数极值法去除极端值\n",
    "def Median(alpha_df,factor_list):\n",
    "    date_list = list(alpha_df[\"datetime\"].drop_duplicates())\n",
    "    grouped = alpha_df.groupby(\"datetime\")\n",
    "\n",
    "    corrected_df = pd.DataFrame()\n",
    "    flag = 0\n",
    "\n",
    "    for date in date_list:\n",
    "        print(date,end=\" \")\n",
    "        try:\n",
    "            panel_data = grouped.get_group(date)\n",
    "        except:\n",
    "            continue\n",
    "        df = pd.DataFrame(panel_data)\n",
    "        colomn_names = factor_list\n",
    "        for colomn_name in colomn_names:\n",
    "            arr = np.array(df[colomn_name])\n",
    "            M = np.median(arr)\n",
    "            MAD = 1.483 * np.median(np.abs(arr - M))\n",
    "            arr[arr > M + 3 * MAD] = M + 3 * MAD\n",
    "            arr[arr < M - 3 * MAD] = M - 3 * MAD\n",
    "            df[colomn_name] = arr\n",
    "        if flag == 0:\n",
    "            flag = 1\n",
    "            corrected_df = df\n",
    "        else:\n",
    "            corrected_df = pd.concat([corrected_df, df])\n",
    "\n",
    "    #corrected_df.to_csv(\"median_mad.csv\", index=False)\n",
    "    return corrected_df\n",
    "\n",
    "alpha_df=df_1\n",
    "factor_list=['BETA','RSTR','LNCAP','ETOP','DASTD','EGRO','BTOP','DTOA','STOM']\n",
    "df_2=Median(alpha_df,factor_list)\n",
    "\n",
    "\n",
    "#Z_score法进行标准化\n",
    "def Z_score(alpha_df,factor_list):\n",
    "    date_list = list(alpha_df[\"datetime\"].drop_duplicates())\n",
    "    grouped = alpha_df.groupby(\"datetime\")\n",
    "    stddized_df = pd.DataFrame()\n",
    "    flag = 0\n",
    "\n",
    "    for date in date_list:\n",
    "        print(date,end=\" \")\n",
    "        try:\n",
    "            panel_data = grouped.get_group(date)\n",
    "        except:\n",
    "            continue\n",
    "        df = pd.DataFrame(panel_data)\n",
    "\n",
    "        colomn_names = factor_list\n",
    "        for colomn_name in colomn_names:\n",
    "            arr = np.array(df[colomn_name])\n",
    "            weight = np.array(len(arr) * [1 / len(arr)])\n",
    "            weighted_avarage = np.dot(weight.T, arr)\n",
    "            arr = (arr - weighted_avarage) / arr.std()\n",
    "            df[colomn_name] = arr\n",
    "        if flag == 0:\n",
    "            flag = 1\n",
    "            stddized_df = df\n",
    "        else:\n",
    "            stddized_df = pd.concat([stddized_df, df])\n",
    "\n",
    "    stddized_df.to_csv(\"equal_weight ZScore.csv\", index=False)\n",
    "\n",
    "\n",
    "factor_list=['BETA','RSTR','LNCAP','ETOP','DASTD','EGRO','BTOP','DTOA','STOM']\n",
    "alpha_df=df_2\n",
    "Z_score(alpha_df,factor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#添加上股票的日频收益率数据\n",
    "zz_data=pd.read_csv(\"equal_weight ZScore.csv\")\n",
    "l=list(set(zz_data[\"code\"][zz_data[\"industry_code\"]==0]))\n",
    "zz_data.set_index(\"code\",inplace=True)\n",
    "zz_data.drop(l,inplace=True)\n",
    "zz_data.reset_index(inplace=True)\n",
    "\n",
    "change_data_1=pd.read_csv(\"change.csv\",dtype={\"stock_code\":str})\n",
    "change_data_2=pd.read_csv(\"change_2.csv\",dtype={\"stock_code\":str})\n",
    "change_data=pd.concat([change_data_1,change_data_2])\n",
    "\n",
    "change_data.rename(columns={\"stock_code\":\"code\",\"trade_date\":\"datetime\"},inplace=True)\n",
    "change_data[\"datetime\"]=change_data[\"datetime\"].apply(lambda x:x[0:10])\n",
    "change_data[\"code\"]=change_data[\"code\"].apply(lambda x:x+\".SH\" if x[0]==\"6\" else x+\".SZ\")\n",
    "zz_data_1=pd.merge(zz_data,change_data,\"left\",on=[\"code\",\"datetime\"])\n",
    "\n",
    "zz_data_1[\"change_rate\"].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d507647b38b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mnew_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzz_data_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"21d_ret\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"avg_ret\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#根据股票日频收益率计算出股票21天收益率以及年化21天平均收益\n",
    "def fun(df):\n",
    "    df=pd.DataFrame(df)\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    df[\"ln_change\"]=np.log(1+df[\"change_rate\"]/100)\n",
    "    df[\"21d_ret\"]=(np.exp(pd.rolling_sum(df[\"ln_change\"],21))-1)*100\n",
    "    df[\"avg_ret\"]=pd.rolling_mean(df[\"21d_ret\"][::-1],200)[::-1].shift(-20)\n",
    "    \n",
    "    index=df[\"index\"]\n",
    "    new_df[\"avg_ret\"][index]=df[\"avg_ret\"]\n",
    "    new_df[\"21d_ret\"][index]=df[\"21d_ret\"]\n",
    "    print(\"finish\",end=\" \")\n",
    "    return 0\n",
    "\n",
    "new_df=pd.DataFrame(zz_data_1)\n",
    "new_df[\"21d_ret\"]=0\n",
    "new_df[\"avg_ret\"]=0\n",
    "zz_data_1.groupby(\"code\").apply(fun)\n",
    "\n",
    "new_df.to_csv(\"equal_weight ZScore.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
