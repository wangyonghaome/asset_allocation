{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from cvxopt import matrix,solvers,blas\n",
    "import cvxopt as opt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#市值平方根加权回归计算每日的因子收益率\n",
    "def factor_return_calcu(alpha_df,factor_list):\n",
    "    grouped=alpha_df.groupby(\"datetime\")\n",
    "    date_list=list(alpha_df[\"datetime\"].drop_duplicates())\n",
    "    code_list=list(alpha_df[\"code\"].drop_duplicates())\n",
    "    factor_names=factor_list\n",
    "\n",
    "\n",
    "    factor_return=pd.DataFrame(index=date_list[:-21],columns=range(len(factor_list)+28))\n",
    "\n",
    "    i = 0\n",
    "    while(1):\n",
    "        print(i)\n",
    "        if i+21>=len(date_list):\n",
    "            break\n",
    "\n",
    "        date=date_list[i]\n",
    "        df=pd.DataFrame(grouped.get_group(date))\n",
    "        sqrt_mktv_array=np.sqrt(np.array(df[\"market_value\"]))\n",
    "\n",
    "        #市值平方根加权OLS回归\n",
    "        return_value = np.array(df[\"21d_ret\"]) * sqrt_mktv_array\n",
    "        \n",
    "        #风格因子\n",
    "        x=np.empty(shape=(len(sqrt_mktv_array),0))\n",
    "        for factor_name in factor_names:\n",
    "            factor_value = np.array(df[factor_name])*sqrt_mktv_array\n",
    "            x=np.column_stack([x,factor_value])\n",
    "        \n",
    "        #行业哑变量\n",
    "        dummy_var = np.array(sm.categorical(np.array(df[\"industry_code\"])))[:, 1:]\n",
    "        for j in range(dummy_var.shape[1]):\n",
    "            dummy_var[:,j]=dummy_var[:,j]*sqrt_mktv_array\n",
    "        x=np.column_stack([x,dummy_var])\n",
    "\n",
    "        model = sm.OLS(return_value, x)\n",
    "        results = model.fit()\n",
    "\n",
    "        factor_return.loc[date,:]=results.params\n",
    "\n",
    "        i = i+1\n",
    "\n",
    "    factor_return.to_csv(\"factor_return.csv\",index_label=\"date\")\n",
    "\n",
    "alpha_df=pd.read_csv(\"equal_weight ZScore.csv\").fillna(0)\n",
    "factor_list=['BETA','RSTR','LNCAP','ETOP','DASTD','EGRO','BTOP','DTOA','STOM']\n",
    "factor_return_calcu(alpha_df,factor_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#市值平方根加权回归计算每日股票特质性收益\n",
    "def spe_return_calcu(alpha_df,factor_list):\n",
    "    grouped = alpha_df.groupby(\"datetime\")\n",
    "    date_list = list(alpha_df[\"datetime\"].drop_duplicates())\n",
    "    code_list = list(alpha_df[\"code\"].drop_duplicates())\n",
    "    factor_names = factor_list\n",
    "\n",
    "    spe_return=pd.DataFrame(index=code_list,columns=date_list[:-21])\n",
    "\n",
    "    i = 0\n",
    "    while(1):\n",
    "        print(i)\n",
    "        if i+21>=len(date_list):\n",
    "            break\n",
    "\n",
    "        date=date_list[i]\n",
    "        df=pd.DataFrame(grouped.get_group(date))\n",
    "        sqrt_mktv_array=np.sqrt(np.array(df[\"market_value\"]))\n",
    "\n",
    "        #市值平方根加权OLS回归\n",
    "        return_value = np.array(df[\"21d_ret\"]) * sqrt_mktv_array\n",
    "        \n",
    "        #风格因子\n",
    "        x = np.empty(shape=(len(sqrt_mktv_array), 0))\n",
    "        for factor_name in factor_names:\n",
    "            factor_value = np.array(df[factor_name])*sqrt_mktv_array\n",
    "            x=np.column_stack([x,factor_value])\n",
    "        \n",
    "        #行业哑变量\n",
    "        dummy_var = np.array(sm.categorical(np.array(df[\"industry_code\"])))[:, 1:]\n",
    "        for j in range(dummy_var.shape[1]):\n",
    "            dummy_var[:,j]=dummy_var[:,j]*sqrt_mktv_array\n",
    "        x=np.column_stack([x,dummy_var])\n",
    "\n",
    "        model = sm.OLS(return_value, x)\n",
    "        results = model.fit()\n",
    "\n",
    "        resid=(return_value-results.fittedvalues)/sqrt_mktv_array\n",
    "        dic=dict(zip(np.array(df[\"code\"]),resid))\n",
    "        spe_return[date]=code_list\n",
    "        spe_return[date]=spe_return[date].apply(lambda x:dic.get(x,0))\n",
    "\n",
    "        i = i+1\n",
    "\n",
    "    spe_return.to_csv(\"spe_return.csv\",index_label=\"code\")\n",
    "\n",
    "alpha_df=pd.read_csv(\"equal_weight ZScoreequal_weight ZScore.csv\").fillna(0)\n",
    "factor_list=['BETA','RSTR','LNCAP','ETOP','DASTD','EGRO','BTOP','DTOA','STOM']\n",
    "spe_return_calcu(alpha_df,factor_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算因子收益的协方差矩阵（原本还进行了New-West调整，但是发现调整之后风险会出现持续高估的情况，最后就把New-West调整舍去了）\n",
    "NW_dict={}\n",
    "\n",
    "def New_West(date_sect,dt):\n",
    "\n",
    "    k=date_sect\n",
    "    date_list = list(dt.index.drop_duplicates())\n",
    "    date=date_list[date_sect]\n",
    "\n",
    "    print(date)\n",
    "    \n",
    "    #计算各时期的权重（时间越近，权重越高）\n",
    "    lamd=pow(0.5,1/90)\n",
    "    weight=np.power(lamd,np.arange(252))\n",
    "    weight=weight/np.sum(weight)\n",
    "    \n",
    "    \n",
    "    #计算因子收益的协方差矩阵\n",
    "    factor_num=len(dt.columns)\n",
    "    dt=np.array(dt)\n",
    "    def f1(index):\n",
    "        i=int(index/factor_num)\n",
    "        j=int(index%factor_num)\n",
    "        f_1 = dt[k + 21:k + 273, i]\n",
    "        f_2 = dt[k + 21:k + 273, j]\n",
    "        return np.sum(weight*(f_1-f_1.mean())*(f_2-f_2.mean()))#+2/3*np.sum(weight[:-1] * (f_1[1:] - f_1.mean()) * (f_2[:-1] - f_2.mean()))\\\n",
    "               #+2/3*np.sum(weight[:-1] * (f_1[:-1] - f_1.mean()) * (f_2[1:] - f_2.mean()))+1/3*np.sum(weight[:-2] * (f_1[2:] - f_1.mean()) * (f_2[:-2] - f_2.mean()))\\\n",
    "               #+1/3*np.sum(weight[:-2] * (f_1[:-2] - f_1.mean()) * (f_2[2:] - f_2.mean()))\n",
    "    pf1=np.vectorize(f1,otypes=[float])\n",
    "    F_NW=pf1(np.arange(factor_num**2)).reshape(factor_num,factor_num)\n",
    "    \n",
    "    #将因子收益协方差矩阵放入字典中\n",
    "    NW_dict[date]=F_NW\n",
    "    print(\"finish\")\n",
    "\n",
    "    return 0\n",
    "\n",
    "data=pd.read_csv(\"factor_return.csv\",engine=\"python\",encoding=\"utf-8\")\n",
    "data.set_index(\"date\",inplace=True)\n",
    "for i in range(len(data.index)-294):\n",
    "    New_West(i,data)\n",
    "\n",
    "print(NW_dict)\n",
    "f=open(\"New_West.txt\",\"wb\")\n",
    "pickle.dump(NW_dict,f,0)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算股票特质性收益的波动率（同样是原本还进行了New-West调整，但是发现调整之后风险会出现持续高估的情况，最后就把New-West调整舍去了）\n",
    "NW_dict={}\n",
    "\n",
    "def spec_NW(date_sect,data):\n",
    "    print(date_sect)\n",
    "    k = date_sect\n",
    "    date_list = list(data.columns.drop_duplicates())\n",
    "    date = date_list[date_sect]\n",
    "    \n",
    "    #计算各个日期的权重（时间越近，权重越高）\n",
    "    lamd=pow(0.5,1/90)\n",
    "    weight=np.power(lamd,np.arange(252))\n",
    "    weight=weight/np.sum(weight)\n",
    "    \n",
    "    #计算股票特质性收益的方差\n",
    "    stock_num=len(data.index)\n",
    "    data = np.array(data)\n",
    "    def f1(i):\n",
    "        f_1 = data[i, k + 21:k + 273]\n",
    "        return np.sum(weight * (f_1 - f_1.mean()) ** 2)\n",
    "    pf1=np.vectorize(f1,otypes=[float])\n",
    "    F_raw=pf1(np.arange(stock_num))\n",
    "\n",
    "    def f2(i):\n",
    "        f_1 = data[i, k + 21:k + 273]\n",
    "        return np.sum(weight[:-1] * (f_1[:-1] - f_1.mean()) * (f_1[1:] - f_1.mean()))\n",
    "    pf2=np.vectorize(f2,otypes=[float])\n",
    "    C_lag_1=pf2(np.arange(stock_num))\n",
    "\n",
    "    def f3(i):\n",
    "        f_1 = data[i, k + 21:k + 273]\n",
    "        return np.sum(weight[:-2] * (f_1[:-2] - f_1.mean()) * (f_1[2:] - f_1.mean()))\n",
    "    pf3=np.vectorize(f3,otypes=[float])\n",
    "    C_lag_2=pf3(np.arange(stock_num))\n",
    "\n",
    "    F_NW=F_raw#+4/3*C_lag_1+2/3*C_lag_2\n",
    "    \n",
    "    #将股票特质性收益的方差存入字典中\n",
    "    NW_dict[date]=F_NW\n",
    "\n",
    "    return F_NW\n",
    "\n",
    "data=pd.read_csv(\"spe_return.csv\",index_col=\"code\").fillna(0)\n",
    "for i in range(len(data.columns)-295):\n",
    "    spec_NW(i,data)\n",
    "\n",
    "print(NW_dict)\n",
    "\n",
    "f=open(\"spec_NW.txt\",\"wb\")\n",
    "pickle.dump(NW_dict,f,0)\n",
    "f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#对因子收益的协方差矩阵进行特征值调整,策略中未使用\n",
    "Eig_dict={}\n",
    "\n",
    "def Eigen_adjust(cov_matrix):\n",
    "    factor_num=cov_matrix.shape[0]\n",
    "    eigvalue,eigvector=np.linalg.eig(cov_matrix)\n",
    "\n",
    "    bias_vec=np.zeros(factor_num,dtype=float)\n",
    "    \n",
    "    #进行1000次模拟，算出平均偏差\n",
    "    for j in range(1000):\n",
    "        eigfactor_matrix=np.empty(shape=(0,252),dtype=float)\n",
    "        for i in range(factor_num):\n",
    "            vec=np.random.normal(0,np.sqrt(eigvalue[i]),252)\n",
    "            eigfactor_matrix=np.row_stack([eigfactor_matrix,vec])\n",
    "\n",
    "        factor_matrix=np.dot(eigvector,eigfactor_matrix)\n",
    "\n",
    "        F_MC=np.cov(factor_matrix)\n",
    "\n",
    "        eigvalue_mc,eigvector_mc=np.linalg.eig(F_MC)\n",
    "\n",
    "        eigvalue_true=np.diag(np.dot(np.dot(eigvector_mc.transpose(),cov_matrix),eigvector_mc))\n",
    "\n",
    "        bias=eigvalue_true/eigvalue_mc\n",
    "\n",
    "        bias_vec+=bias/1000\n",
    "\n",
    "    #对平均偏差进行旋转\n",
    "    bias_vec=np.sqrt(bias_vec)\n",
    "    bias_vec=1.2*(bias_vec-1)+1\n",
    "    bias_vec=np.power(bias_vec,2)\n",
    "    eigvalue_adjust=bias_vec*eigvalue\n",
    "    \n",
    "    #计算出新的协方差矩阵\n",
    "    cov_matrix=np.dot(np.dot(eigvector,np.diag(eigvalue_adjust)),eigvector.transpose())\n",
    "\n",
    "    return cov_matrix\n",
    "\n",
    "f=open(\"New_West.txt\",\"rb\")\n",
    "NW_dict=pickle.load(f)\n",
    "date_list=list(NW_dict.keys())\n",
    "\n",
    "for date in date_list:\n",
    "    print(date)\n",
    "    F_NW=NW_dict[date]\n",
    "    F_Eigen =Eigen_adjust(F_NW)\n",
    "    Eig_dict[date]=F_Eigen\n",
    "\n",
    "f = open(\"Eigen.txt\", \"wb\")\n",
    "pickle.dump(Eig_dict, f, 0)\n",
    "f.close()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#对股票特质性收益进行贝叶斯压缩，压缩目标为所在市值分组平均风险,策略中未使用\n",
    "BS_dict={}\n",
    "\n",
    "def Bayes_shrinkage(specrisk_df):\n",
    "    #按市值排名进行分组\n",
    "    mkt_arr=np.array(specrisk_df[\"market_value\"])\n",
    "    mkt_rank = mkt_arr.argsort().argsort()\n",
    "    specrisk_df[\"mkt_rank\"]=mkt_rank\n",
    "    mkt_tag = pd.cut(specrisk_df.mkt_rank, 10, labels=False)\n",
    "    specrisk_df[\"tag\"] = mkt_tag\n",
    "    \n",
    "    #计算出各组风险的平均值和标准差\n",
    "    avg = specrisk_df[[\"risk\"]].groupby(specrisk_df[\"tag\"]).agg([np.mean])\n",
    "    std = specrisk_df[[\"risk\"]].groupby(specrisk_df[\"tag\"]).agg([np.std])\n",
    "    specrisk_df[\"avg\"]=specrisk_df[\"tag\"].apply(lambda x:avg.iloc[x,0])\n",
    "    specrisk_df[\"std\"] = specrisk_df[\"tag\"].apply(lambda x: std.iloc[x, 0])\n",
    "    \n",
    "    #进行贝叶斯压缩估计（系数q设为1）\n",
    "    q=1\n",
    "    specrisk_df[\"v\"]=np.abs(q*specrisk_df[\"risk\"]-specrisk_df[\"avg\"])/(np.abs(q*(specrisk_df[\"risk\"]-specrisk_df[\"avg\"]))+specrisk_df[\"std\"])\n",
    "    specrisk_df[\"BS_risk\"]=specrisk_df[\"v\"]*specrisk_df[\"avg\"]+(1-specrisk_df[\"v\"])*specrisk_df[\"risk\"]\n",
    "    return list(specrisk_df[\"BS_risk\"])\n",
    "\n",
    "f=open(\"spec_NW.txt\",\"rb\")\n",
    "NW_dict=pickle.load(f)\n",
    "date_list=list(NW_dict.keys())\n",
    "\n",
    "alpha_df=pd.read_csv(\"equal_weight ZScore.csv\")\n",
    "grouped=alpha_df.groupby(\"datetime\")\n",
    "\n",
    "for date in date_list:\n",
    "    print(date)\n",
    "    F_NW=NW_dict[date]\n",
    "    mktv=pd.DataFrame(grouped.get_group(date))[\"market_value\"]\n",
    "    d={\"risk\":np.sqrt(F_NW),\"market_value\":mktv}\n",
    "    specrisk_df=pd.DataFrame(data=d)\n",
    "    BS_risk=Bayes_shrinkage(specrisk_df)\n",
    "    BS_dict[date]=np.power(BS_risk,2)\n",
    "\n",
    "f=open(\"spec_BS.txt\",\"wb\")\n",
    "pickle.dump(BS_dict,f,0)\n",
    "f.close()\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#对因子收益协方差矩阵进行偏误调整,策略中未使用\n",
    "Bias_dict={}\n",
    "\n",
    "f=open(\"Eigen.txt\",\"rb\")\n",
    "Eigen_dict=pickle.load(f)\n",
    "date_list=list(Eigen_dict.keys())\n",
    "date_list.sort(reverse=True)\n",
    "\n",
    "def Bias_adjust(alpha_df,date_sect):\n",
    "    k = date_sect\n",
    "    date = date_list[date_sect]\n",
    "    \n",
    "    #计算各个日期的权重（时间越近，权重越高）\n",
    "    lamd = pow(0.5, 1 / 45)\n",
    "    weight = np.power(lamd, np.arange(126))\n",
    "    weight = weight / np.sum(weight)\n",
    "    \n",
    "    \n",
    "    f_vol_multi=0\n",
    "    for i in range(1,127):\n",
    "        #获得每日风险预测值\n",
    "        _date_=date_list[k+i]\n",
    "        risk=np.sqrt(np.diag(Eigen_dict[_date_]))\n",
    "        \n",
    "        #计算每日的Bias统计量\n",
    "        Bias_t=0\n",
    "        for j in range(len(risk)):\n",
    "            risk_fore=risk[j]\n",
    "            Bias_t+=(alpha_df.iloc[i+k+20,j]/risk_fore)**2\n",
    "        Bias_t/=len(risk)\n",
    "        \n",
    "        f_vol_multi+=Bias_t*weight[i-1]\n",
    "\n",
    "    print(f_vol_multi)\n",
    "    Bias_dict[date]=f_vol_multi*Eigen_dict[date]\n",
    "\n",
    "\n",
    "\n",
    "data=pd.read_csv(\"factor_return.csv\",engine=\"python\",encoding=\"utf-8\")\n",
    "data.set_index(\"date\",inplace=True)\n",
    "for i in range(len(data.index)-427):\n",
    "    print(i)\n",
    "    Bias_adjust(data,i)\n",
    "\n",
    "f=open(\"Biasadjust.txt\",\"wb\")\n",
    "pickle.dump(Bias_dict,f,0)\n",
    "f.close()\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#对股票特质性风险进行偏误调整,策略中未使用\n",
    "Biasadjust_dict = {}\n",
    "\n",
    "f = open(\"spec_BS.txt\", \"rb\")\n",
    "BS_dict = pickle.load(f)\n",
    "date_list = list(BS_dict.keys())\n",
    "date_list.sort(reverse=True)\n",
    "\n",
    "\n",
    "def spec_Bias_adjust(alpha_df, date_sect):\n",
    "    k = date_sect\n",
    "    date = date_list[date_sect]\n",
    "\n",
    "    print(date)\n",
    "    \n",
    "    #计算各个日期的权重（时间越近，权重越高）\n",
    "    lamd = pow(0.5, 1 / 45)\n",
    "    weight = np.power(lamd, np.arange(126))\n",
    "    weight = weight / np.sum(weight)\n",
    "\n",
    "    spe_vol_multi = 0\n",
    "    for i in range(1, 127):\n",
    "        #获得每日的风险预测值\n",
    "        _date_ = date_list[k + i]\n",
    "        risk = np.sqrt(BS_dict[_date_])\n",
    "    \n",
    "        #计算每日的Bias统计量\n",
    "        stock_num = len(alpha_df.index)\n",
    "        stock_range = range(stock_num)\n",
    "        ret = alpha_df.iloc[stock_range, i+k + 20]\n",
    "        Bias_t = np.sum(np.power(ret / risk, 2)) / len(risk)\n",
    "        \n",
    "        spe_vol_multi += Bias_t * weight[i - 1]\n",
    "\n",
    "    print(spe_vol_multi)\n",
    "    Biasadjust_dict[date] = spe_vol_multi * BS_dict[date]\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"spe_return.csv\", index_col='code').fillna(0)\n",
    "for i in range(len(data.columns) - 427):\n",
    "    spec_Bias_adjust(data, i)\n",
    "\n",
    "f = open(\"spec_Biasadjust.txt\", \"wb\")\n",
    "pickle.dump(Biasadjust_dict, f, 0)\n",
    "f.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得基准权重\n",
    "f=open(\"index_w_dict.txt\",\"rb\")\n",
    "index_w_dict=pickle.load(f)\n",
    "\n",
    "weights_dict={}#存放最优化组合权重\n",
    "\n",
    "alpha_df=pd.read_csv(\"equal_weight ZScore.csv\").fillna(0)\n",
    "\n",
    "grouped = alpha_df.groupby(\"datetime\")\n",
    "date_list = list(alpha_df[\"datetime\"].drop_duplicates())\n",
    "code_list = list(alpha_df[\"code\"].drop_duplicates())\n",
    "\n",
    "def optimize(date):\n",
    "    print(date)\n",
    "    df = pd.DataFrame(grouped.get_group(date))\n",
    "    n = len(df[\"code\"])\n",
    "    \n",
    "    #获得当日基准组合的权重\n",
    "    index_weight=index_w_dict[date]\n",
    "    df[\"weight\"]=df[\"code\"].apply(lambda x:index_weight.get(x.replace(\"SH\",\"XSHG\"),0) if \"SH\" in x\\\n",
    "                                            else index_weight.get(x.replace(\"SZ\",\"XSHE\"),0))\n",
    "    df[\"weight\"]=df[\"weight\"]*1/sum(df[\"weight\"])\n",
    "\n",
    "    factor_list =['BETA','LNCAP','RSTR','ETOP','DASTD','EGRO','BTOP','DTOA','STOM']\n",
    "    X = np.empty(shape=[len(df[\"code\"]), 0])\n",
    "    for factor_name in factor_list:\n",
    "        factor_data = np.array(df[factor_name])\n",
    "        X = np.column_stack([X, factor_data])\n",
    "    X = X.transpose()#风格因子暴露矩阵\n",
    "\n",
    "\n",
    "    indus_var = np.array(sm.categorical(np.array(df[\"industry_code\"])))[:, 1:]\n",
    "    indus_var = np.array(indus_var).transpose()#行业因子暴露矩阵\n",
    "\n",
    "    mtx=np.row_stack([X,indus_var])#所有股票在风格因子和行业因子上的暴露矩阵\n",
    "\n",
    "    f=open(\"New_West.txt\",\"rb\")\n",
    "    factor_cov=pickle.load(f)[date]\n",
    "    cov_matrix=np.dot(np.dot(mtx.transpose(),factor_cov),mtx)#结构化风险协方差矩阵\n",
    "    f=open(\"spec_NW.txt\",\"rb\")\n",
    "    spe_risk=pickle.load(f)[date]#每个股票的特质性风险\n",
    "    cov_matrix+=np.diag(spe_risk)\n",
    "    \n",
    "    #最小化的函数\n",
    "    P = matrix(cov_matrix)\n",
    "    q = -matrix(df[\"avg_ret\"])\n",
    "\n",
    "    #约束权重和为1\n",
    "    A = matrix(1.0, (1, n))\n",
    "    b = matrix(1.0)\n",
    "\n",
    "\n",
    "    I = matrix(0.0, (n, n))\n",
    "    I[::n + 1] = 1.0\n",
    "    G_1 = matrix([-I, I])#约束权重上下限\n",
    "    factor = opt.matrix(mtx[[0,2],:])\n",
    "    G_2=matrix([-factor,factor])#约束风格因子和行业因子暴露值上下限\n",
    "    G=matrix([G_1,G_2])\n",
    "    h_1 = matrix([opt.matrix(n*[0]),opt.matrix(df[\"weight\"]+0.015)])#约束权重上下限\n",
    "    bench = factor * matrix(df[\"weight\"])\n",
    "    h_2_high=opt.matrix([x*1.2 if x>0 else x*0.8 for x in bench])\n",
    "    h_2_low=opt.matrix([x*(-0.8) if x >0 else x*(-1.2) for x in bench])\n",
    "    h_2=matrix([h_2_low,h_2_high])#约束风格因子和行业因子暴露值上下限\n",
    "    h=matrix([h_1,h_2])\n",
    "\n",
    "    dims = {\"l\": G.size[0], \"q\": [], \"s\": []}\n",
    "    portfolio = solvers.coneqp(P, q, G, h, dims,A,b)[\"x\"]#最优化计算\n",
    "    returns = blas.dot(q, portfolio)#收益率\n",
    "    risks = np.sqrt(blas.dot(portfolio, P * portfolio))#结构化风险\n",
    "    portfolio=np.array(portfolio).flatten()\n",
    "    \n",
    "    #打印出每日持仓组合以及收益风险情况\n",
    "    print(portfolio[portfolio>0.00001])\n",
    "    print(returns,\" \",risks)\n",
    "\n",
    "    port_dict=dict(zip(df[\"code\"],portfolio))\n",
    "    weights_dict[date]=port_dict\n",
    "\n",
    "    return 0\n",
    "\n",
    "f=open(r\"day.txt\",\"rb\")\n",
    "dt_list=pickle.load(f)\n",
    "for date in dt_list[3:]:\n",
    "    optimize(date)\n",
    "\n",
    "f=open(\"weights_dict_3.txt\",\"wb\")\n",
    "pickle.dump(weights_dict,f,0)\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
