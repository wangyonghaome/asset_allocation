{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import scipy.optimize as sco\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算最优化权重的函数\n",
    "\n",
    "#风险平价优化\n",
    "def obj_fun_1(weight,cov_mat):\n",
    "    arr_1=np.dot(cov_mat,weight)\n",
    "    arr_2=weight*arr_1\n",
    "\n",
    "    s = 0\n",
    "    for x_1 in arr_2:\n",
    "        for x_2 in arr_2:\n",
    "            s+=(x_1-x_2)**2\n",
    "    return s\n",
    "    \n",
    "       \n",
    "def opt_1(cov_mat):\n",
    "    num=cov_mat.shape[0]\n",
    "    cons=({\"type\":\"eq\",\"fun\":lambda x:np.sum(x)-1})\n",
    "    bound=tuple((0,1) for x in range(num))\n",
    "    #options={'disp':False, 'maxiter':1000, 'ftol':1e-20}\n",
    "    opt_weight=sco.minimize(obj_fun_1,num*[1/num],args=cov_mat,method=\"SLSQP\",bounds=bound,constraints=cons)[\"x\"]\n",
    "    return opt_weight \n",
    "\n",
    "#最小化方差优化\n",
    "def obj_fun_2(weight,cov_mat):\n",
    "    variance=np.dot(np.dot(weight.T,cov_mat),weight)\n",
    "    return variance\n",
    "\n",
    "def opt_2(cov_mat):\n",
    "    num=cov_mat.shape[0]\n",
    "    cons=({\"type\":\"eq\",\"fun\":lambda x:np.sum(x)-1})\n",
    "    bound=tuple((0,1) for x in range(num))\n",
    "    options={'disp':False, 'maxiter':1000, 'ftol':1e-20}\n",
    "    opt_weight=sco.minimize(obj_fun_2,num*[1/num],args=cov_mat,method=\"SLSQP\",bounds=bound,constraints=cons,options=options)[\"x\"]\n",
    "    return opt_weight\n",
    "\n",
    "#均值方差优化\n",
    "def obj_fun_4(weight,ret_arr,cov_mat):\n",
    "    variance=np.dot(np.dot(weight.T,cov_mat),weight)\n",
    "    ret = np.sum(ret_arr*weight)\n",
    "    return 0.5*variance-ret\n",
    "\n",
    "def opt_4(ret_arr,cov_mat):\n",
    "    num=cov_mat.shape[0]\n",
    "    cons=({\"type\":\"eq\",\"fun\":lambda x:np.sum(x)-1})\n",
    "    bound=tuple((0,1) for x in range(num))\n",
    "    #options={'disp':False, 'maxiter':1000, 'ftol':1e-20}\n",
    "    opt_weight=sco.minimize(obj_fun_4,num*[1/num],args=(ret_arr,cov_mat),method=\"SLSQP\",bounds=bound,constraints=cons)[\"x\"]\n",
    "    return opt_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读入各行业日频收益率数据并进行形式上的处理\n",
    "data_df=pd.read_csv(\"indus_ret.csv\")\n",
    "data_df.sort_values(by=[\"trade_date\",\"industry_code\"],ascending=[\"True\",\"True\"],inplace=True)\n",
    "\n",
    "data_df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "grouped=data_df.groupby(\"trade_date\")\n",
    "date_list=list(data_df[\"trade_date\"].drop_duplicates())[:-39]\n",
    "code_list=list(data_df[\"industry_code\"].drop_duplicates())\n",
    "\n",
    "ret_arr=np.empty(shape=(len(code_list),0))\n",
    "\n",
    "for date in date_list:\n",
    "    change_rate=np.array(grouped.get_group(date)[\"change_rate\"])\n",
    "    ret_arr=np.column_stack([ret_arr,change_rate])\n",
    "\n",
    "indus_ret_df=pd.DataFrame(ret_arr.T)\n",
    "\n",
    "for i in range(28):\n",
    "    indus_ret_df[str(i)+\"_In_change\"]=np.array(np.log(1+indus_ret_df[i]/100))[::-1]\n",
    "    indus_ret_df[str(i)+\"_indus_7d_ret\"]=(np.array(np.exp(indus_ret_df[str(i)+\"_In_change\"].rolling(7).apply(np.sum)))[::-1]-1)*100\n",
    "    del indus_ret_df[str(i)+\"_In_change\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算Newey_West调整后的协方差矩阵\n",
    "def Newey_West(arr,half_time):\n",
    "    num=arr.shape[1]\n",
    "    \n",
    "    lamd=pow(0.5,1/half_time)\n",
    "    weight=np.power(lamd,np.arange(252))[::-1]\n",
    "    weight=weight/np.sum(weight)\n",
    "    \n",
    "    def f1(index):\n",
    "        i=int(index/num)\n",
    "        j=int(index%num)\n",
    "        f_1 = arr[:, i]\n",
    "        f_2 = arr[:, j]\n",
    "        return np.sum(weight*(f_1-np.sum(weight*f_1))*(f_2-np.sum(weight*f_2)))+2/3*np.sum(weight[1:] * (f_1[1:] - np.sum(weight[1:]*f_1[1:])) * (f_2[:-1] - np.sum(weight[1:]*f_2[:-1])))\\\n",
    "               +2/3*np.sum(weight[1:] * (f_1[:-1] - np.sum(weight[1:]*f_1[:-1])) * (f_2[1:] - np.sum(weight[1:]*f_2[1:])))+1/3*np.sum(weight[2:] * (f_1[2:] - np.sum(weight[2:]*f_1[2:])) * (f_2[:-2] - np.sum(weight[2:]*f_2[:-2])))\\\n",
    "               +1/3*np.sum(weight[2:] * (f_1[:-2] - np.sum(weight[2:]*f_1[:-2])) * (f_2[2:] - np.sum(weight[2:]*f_2[2:])))\n",
    "    pf1=np.vectorize(f1,otypes=[float])\n",
    "    F_NW=pf1(np.arange(num**2)).reshape(num,num)\n",
    "    \n",
    "    return F_NW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对Newey_West调整后的协方差矩阵进行特征值调整\n",
    "def Eigen_adjust(cov_matrix):\n",
    "    factor_num=cov_matrix.shape[0]\n",
    "    eigvalue,eigvector=np.linalg.eig(cov_matrix)\n",
    "\n",
    "    bias_vec=np.zeros(factor_num,dtype=float)\n",
    "    \n",
    "    #进行1000次模拟，算出平均偏差\n",
    "    for j in range(1000):\n",
    "        eigfactor_matrix=np.empty(shape=(0,252),dtype=float)\n",
    "        for i in range(factor_num):\n",
    "            vec=np.random.normal(0,np.sqrt(eigvalue[i]),252)\n",
    "            eigfactor_matrix=np.row_stack([eigfactor_matrix,vec])\n",
    "\n",
    "        factor_matrix=np.dot(eigvector,eigfactor_matrix)\n",
    "\n",
    "        F_MC=np.cov(factor_matrix)\n",
    "\n",
    "        eigvalue_mc,eigvector_mc=np.linalg.eig(F_MC)\n",
    "\n",
    "        eigvalue_true=np.diag(np.dot(np.dot(eigvector_mc.transpose(),cov_matrix),eigvector_mc))\n",
    "\n",
    "        bias=eigvalue_true/eigvalue_mc\n",
    "\n",
    "        bias_vec+=bias/1000\n",
    "\n",
    "    #对平均偏差进行旋转\n",
    "    bias_vec=np.sqrt(bias_vec)\n",
    "    bias_vec=1.2*(bias_vec-1)+1\n",
    "    bias_vec=np.power(bias_vec,2)\n",
    "    eigvalue_adjust=bias_vec*eigvalue\n",
    "    #print(bias_vec)\n",
    "    #print(eigvalue)\n",
    "    #print(eigvalue_adjust)\n",
    "    \n",
    "    #计算出新的协方差矩阵\n",
    "    new_cov_matrix=np.dot(np.dot(eigvector,np.diag(eigvalue_adjust)),eigvector.transpose())\n",
    "    \n",
    "    return new_cov_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对特征值调整后的协方差矩阵进行偏误调整\n",
    "def Bias_adjust(ret_df,date):\n",
    "    #计算各个日期的权重（时间越近，权重越高）\n",
    "    lamd = pow(0.5, 1 / 45)\n",
    "    weight = np.power(lamd, np.arange(126))[::-1]\n",
    "    weight = weight / np.sum(weight)\n",
    "    \n",
    "    \n",
    "    f_vol_multi=0\n",
    "    for i in range(126):\n",
    "        #获得每日风险预测值\n",
    "        _date_=date-i\n",
    "        risk=np.sqrt(np.diag(Eigen_dict[_date_]))\n",
    "        \n",
    "        #计算每日的Bias统计量\n",
    "        Bias_t=0\n",
    "        for j in range(len(risk)):\n",
    "            risk_fore=risk[j]\n",
    "            Bias_t+=(ret_df.iloc[date-i,j]/risk_fore)**2\n",
    "        Bias_t/=len(risk)\n",
    "        \n",
    "        f_vol_multi+=Bias_t*weight[i]\n",
    "\n",
    "    #print(f_vol_multi)\n",
    "    Bias_dict[date]=f_vol_multi*Eigen_dict[date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#对不同调整阶段的协方差矩阵进行风险平价优化\n",
    "date_list=list(indus_ret_df.index)\n",
    "weight_dict_1={}\n",
    "weight_dict_2={}\n",
    "weight_dict_3={}\n",
    "weight_dict_4={}\n",
    "\n",
    "ret_df=indus_ret_df\n",
    "Raw_dict={}\n",
    "NW_dict={}\n",
    "Eigen_dict={}\n",
    "for date in range(252,len(date_list)):\n",
    "    print(date,end=\" \")\n",
    "    df=pd.DataFrame(ret_df.iloc[date-252:date,0:28])\n",
    "    \n",
    "    Raw_dict[date]=np.array(df.cov())\n",
    "    \n",
    "    cov_matrix=Newey_West(np.array(df),180)\n",
    "    NW_dict[date]=cov_matrix\n",
    "    cov_matrix=Eigen_adjust(cov_matrix)\n",
    "    Eigen_dict[date]=cov_matrix\n",
    "   \n",
    "Bias_dict={}\n",
    "for i in range(378,len(date_list),7):\n",
    "    Bias_adjust(ret_df,i)\n",
    "\n",
    "for date in range(378,len(date_list),7):\n",
    "    print(\"optimize \",date)\n",
    "    ret_arr=np.array(ret_df.iloc[date-7,28:56])/7\n",
    "    cov_matrix_1=Raw_dict[date]\n",
    "    weight_dict_1[date]=opt_1(cov_matrix_1)\n",
    "    \n",
    "    cov_matrix_2=NW_dict[date]\n",
    "    weight_dict_2[date]=opt_1(cov_matrix_2)\n",
    "    \n",
    "    cov_matrix_3=Eigen_dict[date]\n",
    "    weight_dict_3[date]=opt_1(cov_matrix_3)\n",
    "    \n",
    "    cov_matrix_4=Bias_dict[date]\n",
    "    weight_dict_4[date]=opt_1(cov_matrix_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对不同调整阶段的协方差矩阵进行均值方差优化\n",
    "date_list=list(indus_ret_df.index)\n",
    "weight_dict_1={}\n",
    "weight_dict_2={}\n",
    "weight_dict_3={}\n",
    "weight_dict_4={}\n",
    "\n",
    "ret_df=indus_ret_df\n",
    "Raw_dict={}\n",
    "NW_dict={}\n",
    "Eigen_dict={}\n",
    "for date in range(252,len(date_list)):\n",
    "    print(date,end=\" \")\n",
    "    df=pd.DataFrame(ret_df.iloc[date-252:date,0:28])\n",
    "    \n",
    "    Raw_dict[date]=np.array(df.cov())\n",
    "    \n",
    "    cov_matrix=Newey_West(np.array(df),180)\n",
    "    NW_dict[date]=cov_matrix\n",
    "    cov_matrix=Eigen_adjust(cov_matrix)\n",
    "    Eigen_dict[date]=cov_matrix\n",
    "   \n",
    "Bias_dict={}\n",
    "for i in range(378,len(date_list),7):\n",
    "    Bias_adjust(ret_df,i)\n",
    "\n",
    "for date in range(378,len(date_list),7):\n",
    "    print(\"optimize \",date)\n",
    "    ret_arr=np.array(ret_df.iloc[date-7,28:56])/7\n",
    "    cov_matrix_1=Raw_dict[date]\n",
    "    weight_dict_1[date]=opt_4(ret_arr,cov_matrix_1)\n",
    "    \n",
    "    cov_matrix_2=NW_dict[date]\n",
    "    weight_dict_2[date]=opt_4(ret_arr,cov_matrix_2)\n",
    "    \n",
    "    cov_matrix_3=Eigen_dict[date]\n",
    "    weight_dict_3[date]=opt_4(ret_arr,cov_matrix_3)\n",
    "    \n",
    "    cov_matrix_4=Bias_dict[date]\n",
    "    weight_dict_4[date]=opt_4(ret_arr,cov_matrix_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#组合优化结果展示图\n",
    "\n",
    "zz500=pd.read_csv(\"zz500.csv\")\n",
    "zz500[\"In_change\"]=np.array(np.log(1+zz500[\"change_rate\"]/100))[::-1]\n",
    "zz500[\"mkt_21d_ret\"]=(np.array(np.exp(zz500[\"In_change\"].rolling(7).apply(np.sum)))[::-1]-1)*100\n",
    "\n",
    "mkt_ret_arr=np.array(zz500[\"mkt_21d_ret\"])[504::7]\n",
    "cum_mkt_arr=np.cumprod(1+mkt_ret_arr/100)\n",
    "plt.plot(cum_mkt_arr,color=\"blue\",label=\"bench_mark\")\n",
    "\n",
    "port_ret_ls1=[]\n",
    "port_ret_ls2=[]\n",
    "port_ret_ls3=[]\n",
    "port_ret_ls4=[]\n",
    "port_ret_ls5=[]\n",
    "\n",
    "for date in range(504,len(date_list),7):\n",
    "    ret_arr=(ret_df.iloc[date,28:56])\n",
    "    \n",
    "    port_ret1=np.sum(weight_dict_1[date]*ret_arr)\n",
    "    port_ret2=np.sum(weight_dict_2[date]*ret_arr)\n",
    "    port_ret3=np.sum(weight_dict_3[date]*ret_arr)\n",
    "    port_ret4=np.sum(weight_dict_4[date]*ret_arr)\n",
    "    port_ret5=np.mean(ret_arr)\n",
    "    \n",
    "    port_ret_ls1.append(port_ret1)\n",
    "    port_ret_ls2.append(port_ret2)\n",
    "    port_ret_ls3.append(port_ret3)\n",
    "    port_ret_ls4.append(port_ret4)\n",
    "    port_ret_ls5.append(port_ret5)\n",
    "    \n",
    "port_ret_arr1=np.array(port_ret_ls1)\n",
    "cum_arr1=np.cumprod(1+port_ret_arr1/100)\n",
    "port_ret_arr2=np.array(port_ret_ls2)\n",
    "cum_arr2=np.cumprod(1+port_ret_arr2/100)\n",
    "port_ret_arr3=np.array(port_ret_ls3)\n",
    "cum_arr3=np.cumprod(1+port_ret_arr3/100)\n",
    "port_ret_arr4=np.array(port_ret_ls4)\n",
    "cum_arr4=np.cumprod(1+port_ret_arr4/100)\n",
    "port_ret_arr5=np.array(port_ret_ls5)\n",
    "cum_arr5=np.cumprod(1+port_ret_arr5/100)\n",
    "\n",
    "#绝对收益图\n",
    "plt.plot(cum_arr1,color=\"red\",label=\"Raw\")\n",
    "plt.plot(cum_arr2,color=\"green\",label=\"Newey_West\")\n",
    "plt.plot(cum_arr3,color=\"black\",label=\"Eigen_adjust\")\n",
    "plt.plot(cum_arr4,color=\"brown\",label=\"Bias_adjust\")\n",
    "plt.plot(cum_arr5,color=\"purple\",label=\"equal_weight\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plot_abso.jpg\",dpi=500)\n",
    "plt.show()\n",
    "\n",
    "#相对收益图\n",
    "relative_arr1=cum_arr1/cum_mkt_arr\n",
    "relative_arr2=cum_arr2/cum_mkt_arr\n",
    "relative_arr3=cum_arr3/cum_mkt_arr\n",
    "relative_arr4=cum_arr4/cum_mkt_arr\n",
    "relative_arr5=cum_arr5/cum_mkt_arr\n",
    "plt.plot(relative_arr1,color=\"red\",label=\"Raw\")\n",
    "plt.plot(relative_arr2,color=\"green\",label=\"Newey_West\")\n",
    "plt.plot(relative_arr3,color=\"black\",label=\"Eigen_adjust\")\n",
    "plt.plot(relative_arr4,color=\"brown\",label=\"Bias_adjust\")\n",
    "plt.plot(relative_arr5,color=\"purple\",label=\"equal_weight\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plot_rela.jpg\",dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#组合优化指标对比表格\n",
    "\n",
    "def indicator_calcu(arr):\n",
    "    vola=np.std(arr)\n",
    "    cum_arr=np.cumprod(1+arr/100)\n",
    "    max_retract=(np.max(cum_arr)-np.min(cum_arr[np.argmax(cum_arr):]))/np.max(cum_arr)\n",
    "    sharpe=np.mean(arr)/np.std(arr)\n",
    "    \n",
    "    year_ret=(cum_arr[-1]-1)*100*36/len(cum_arr)\n",
    "    \n",
    "    return year_ret,vola,max_retract,sharpe\n",
    "\n",
    "df=pd.DataFrame(index=[\"Raw\",\"Newey_West\",\"Eigen_adjust\",\"Bias_adjust\",\"equal_weight\"],columns=[\"yearly_ret\",\"vloa\",\"max_retract\",\"sharpe\"])\n",
    "df.iloc[0,:]=list(indicator_calcu(port_ret_arr1[:-1]))\n",
    "df.iloc[1,:]=list(indicator_calcu(port_ret_arr2[:-1]))\n",
    "df.iloc[2,:]=list(indicator_calcu(port_ret_arr3[:-1]))\n",
    "df.iloc[3,:]=list(indicator_calcu(port_ret_arr4[:-1]))\n",
    "df.iloc[4,:]=list(indicator_calcu(port_ret_arr5[:-1]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取股票收益数据并随机抽取500只股票进行计算\n",
    "alpha_df=pd.read_csv(\"equal_weight ZScore_1.csv\")\n",
    "\n",
    "code_list=list(alpha_df[\"code\"].drop_duplicates())\n",
    "alpha_df.set_index(\"code\",inplace=True)\n",
    "drop_code=random.sample(code_list,1635)\n",
    "alpha_df.drop(drop_code,inplace=True)\n",
    "alpha_df.reset_index()\n",
    "\n",
    "alpha_df.sort_values(by=[\"datetime\",\"code\"],ascending=[True,True],inplace=True)\n",
    "ret_df=alpha_df.pivot_table(\"change\",index=\"datetime\",columns=\"code\")\n",
    "mkt_value_df=alpha_df.pivot_table(\"market_value\",index=\"datetime\",columns=\"code\")\n",
    "stock_num=len(ret_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zjh(arr):\n",
    "    arr_t=arr.transpose()\n",
    "\n",
    "    #计算重叠矩阵\n",
    "    overlap_matrix=np.dot(arr_t,arr)\n",
    "    overlap_matrix=(overlap_matrix.T+overlap_matrix)/2\n",
    "    #计算特征值、特征向量\n",
    "    eigvalue,eigvector=np.linalg.eig(overlap_matrix)\n",
    "\n",
    "    #处理对角矩阵\n",
    "    eigvalue=eigvalue**(-0.5)\n",
    "    DiagMat=np.diag(eigvalue)\n",
    "\n",
    "    #计算最终正交矩阵\n",
    "    tmp=np.dot(eigvector,DiagMat)\n",
    "    TSMat=np.dot(tmp,eigvector.transpose())\n",
    "    OrthMat=np.dot(arr,TSMat)\n",
    "    \n",
    "    return OrthMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##计算Newey_West调整后的特质性风险\n",
    "def spec_NW(arr,half_time):\n",
    "    #计算各个日期的权重（时间越近，权重越高）\n",
    "    lamd=pow(0.5,1/half_time)\n",
    "    weight=np.power(lamd,np.arange(252))[::-1]\n",
    "    weight=weight/np.sum(weight)\n",
    "    \n",
    "    #计算股票特质性收益的方差\n",
    "    stock_num=arr.shape[1]\n",
    "    def f1(i):\n",
    "        f_1 = arr[:,i]\n",
    "        return np.sum(weight * (f_1 - np.sum(weight*f_1)) ** 2)\n",
    "    pf1=np.vectorize(f1,otypes=[float])\n",
    "    F_raw=pf1(np.arange(stock_num))\n",
    "\n",
    "    def f2(i):\n",
    "        f_1 = arr[:,i]\n",
    "        return np.sum(weight[1:] * (f_1[:-1] - np.sum(weight[1:]*f_1[:-1])) * (f_1[1:] - np.sum(weight[1:]*f_1[1:])))\n",
    "    pf2=np.vectorize(f2,otypes=[float])\n",
    "    C_lag_1=pf2(np.arange(stock_num))\n",
    "\n",
    "    def f3(i):\n",
    "        f_1 = arr[:,i]\n",
    "        return np.sum(weight[2:] * (f_1[:-2] -  np.sum(weight[2:]*f_1[:-2])) * (f_1[2:] - np.sum(weight[2:]*f_1[2:])))\n",
    "    pf3=np.vectorize(f3,otypes=[float])\n",
    "    C_lag_2=pf3(np.arange(stock_num))\n",
    "\n",
    "    F_NW=F_raw+4/3*C_lag_1+2/3*C_lag_2\n",
    "\n",
    "    return F_NW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对Newey_West调整后的特质性风险进行贝叶斯压缩调整\n",
    "def Bayes_shrinkage(F_NW,mkt_value_df):\n",
    "    mktv=mkt_value_df.iloc[date,:]\n",
    "    d={\"risk\":np.sqrt(F_NW),\"market_value\":mktv}\n",
    "    specrisk_df=pd.DataFrame(data=d)\n",
    "    \n",
    "    #按市值排名进行分组\n",
    "    mkt_arr=np.array(specrisk_df[\"market_value\"])\n",
    "    mkt_rank = mkt_arr.argsort().argsort()\n",
    "    specrisk_df[\"mkt_rank\"]=mkt_rank\n",
    "    mkt_tag = pd.cut(specrisk_df.mkt_rank, 10, labels=False)\n",
    "    specrisk_df[\"tag\"] = mkt_tag\n",
    "    \n",
    "    #计算出各组风险的平均值和标准差\n",
    "    avg = specrisk_df[[\"risk\"]].groupby(specrisk_df[\"tag\"]).agg([np.mean])\n",
    "    std = specrisk_df[[\"risk\"]].groupby(specrisk_df[\"tag\"]).agg([np.std])\n",
    "    specrisk_df[\"avg\"]=specrisk_df[\"tag\"].apply(lambda x:avg.iloc[x,0])\n",
    "    specrisk_df[\"std\"] = specrisk_df[\"tag\"].apply(lambda x: std.iloc[x, 0])\n",
    "    \n",
    "    #进行贝叶斯压缩估计（系数q设为1）\n",
    "    q=1\n",
    "    specrisk_df[\"v\"]=q*np.abs(specrisk_df[\"risk\"]-specrisk_df[\"avg\"])/(np.abs(q*(specrisk_df[\"risk\"]-specrisk_df[\"avg\"]))+specrisk_df[\"std\"])\n",
    "    specrisk_df[\"BS_risk\"]=specrisk_df[\"v\"]*specrisk_df[\"avg\"]+(1-specrisk_df[\"v\"])*specrisk_df[\"risk\"]\n",
    "    return np.power(np.array(specrisk_df[\"BS_risk\"]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对贝叶斯压缩调整后的股票特质性收益进行偏误调整\n",
    "def spec_Bias_adjust(ret_df,date):\n",
    "    #计算各个日期的权重（时间越近，权重越高）\n",
    "    lamd = pow(0.5, 1 / 45)\n",
    "    weight = np.power(lamd, np.arange(126))[::-1]\n",
    "    weight = weight / np.sum(weight)\n",
    "    \n",
    "    \n",
    "    f_vol_multi=0\n",
    "    for i in range(126):\n",
    "        #获得每日风险预测值\n",
    "        _date_=date-i\n",
    "        risk=np.sqrt(BS_dict[date])\n",
    "        \n",
    "        #计算每日的Bias统计量\n",
    "        Bias_t=0\n",
    "        j=np.arange(len(risk))\n",
    "        Bias_t=np.sum(np.power(ret_df.iloc[date-i,j]/risk,2))/len(risk)\n",
    "        \n",
    "        f_vol_multi+=Bias_t*weight[i]\n",
    "\n",
    "    print(f_vol_multi)\n",
    "    Bias_dict[date]=f_vol_multi*BS_dict[date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#对不同调整阶段的特质性风险进行风险平价优化\n",
    "date_list=list(ret_df.index)\n",
    "weight_dict_1={}\n",
    "weight_dict_2={}\n",
    "weight_dict_3={}\n",
    "weight_dict_4={}\n",
    "\n",
    "Raw_dict={}\n",
    "NW_dict={}\n",
    "BS_dict={}\n",
    "\n",
    "ret_df=pd.DataFrame(zjh(np.array(ret_df)))\n",
    "\n",
    "for date in range(252,len(date_list)):\n",
    "    print(date,end=\" \")\n",
    "    df=pd.DataFrame(ret_df.iloc[date-252:date,0:stock_num])\n",
    "    \n",
    "    Raw_dict[date]=np.diag(np.array(df.cov()))\n",
    "    \n",
    "    spec_risk=spec_NW(np.array(df),180)\n",
    "    NW_dict[date]=spec_risk\n",
    "    spec_risk=Bayes_shrinkage(spec_risk,mkt_value_df)\n",
    "    BS_dict[date]=spec_risk\n",
    "\n",
    "    \n",
    "    \n",
    "for code in ret_df.columns:\n",
    "    ret_df[code+\"_In_change\"]=np.array(np.log(1+ret_df[code]/100))[::-1]\n",
    "    ret_df[code+\"_7d_ret\"]=(np.array(np.exp(ret_df[code+\"_In_change\"].rolling(7).apply(np.sum)))[::-1]-1)*100\n",
    "    del ret_df[code+\"_In_change\"]\n",
    "    \n",
    "    \n",
    "Bias_dict={}\n",
    "for i in range(378,len(date_list),7):\n",
    "    spec_Bias_adjust(ret_df,i)\n",
    "\n",
    "for date in range(378,len(date_list),7):\n",
    "    print(\"optimize \",date)\n",
    "    spec_risk_1=Raw_dict[date]\n",
    "    weight_dict_1[date]=spec_risk_1**(-0.5)/np.sum(spec_risk_1**(-0.5))\n",
    "    \n",
    "    spec_risk_2=NW_dict[date]\n",
    "    weight_dict_2[date]=spec_risk_2**(-0.5)/np.sum(spec_risk_2**(-0.5))\n",
    "    \n",
    "    spec_risk_3=BS_dict[date]\n",
    "    weight_dict_3[date]=spec_risk_3**(-0.5)/np.sum(spec_risk_3**(-0.5))\n",
    "    \n",
    "    spec_risk_4=Bias_dict[date]\n",
    "    weight_dict_4[date]=spec_risk_4**(-0.5)/np.sum(spec_risk_4**(-0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#组合优化结果展示图\n",
    "\n",
    "zz500=pd.read_csv(\"zz500.csv\")\n",
    "zz500[\"In_change\"]=np.array(np.log(1+zz500[\"change_rate\"]/100))[::-1]\n",
    "zz500[\"mkt_21d_ret\"]=(np.array(np.exp(zz500[\"In_change\"].rolling(7).apply(np.sum)))[::-1]-1)*100\n",
    "\n",
    "mkt_ret_arr=np.array(zz500[\"mkt_21d_ret\"])[504::7]\n",
    "cum_mkt_arr=np.cumprod(1+mkt_ret_arr/100)\n",
    "plt.plot(cum_mkt_arr,color=\"blue\",label=\"bench_mark\")\n",
    "\n",
    "port_ret_ls1=[]\n",
    "port_ret_ls2=[]\n",
    "port_ret_ls3=[]\n",
    "port_ret_ls4=[]\n",
    "port_ret_ls5=[]\n",
    "\n",
    "for date in range(504,len(date_list),7):\n",
    "    ret_arr=(ret_df.iloc[date,stock_num:2*stock_num])\n",
    "    \n",
    "    port_ret1=np.sum(weight_dict_1[date]*ret_arr)\n",
    "    port_ret2=np.sum(weight_dict_2[date]*ret_arr)\n",
    "    port_ret3=np.sum(weight_dict_3[date]*ret_arr)\n",
    "    port_ret4=np.sum(weight_dict_4[date]*ret_arr)\n",
    "    port_ret5=np.mean(ret_arr)\n",
    "    \n",
    "    port_ret_ls1.append(port_ret1)\n",
    "    port_ret_ls2.append(port_ret2)\n",
    "    port_ret_ls3.append(port_ret3)\n",
    "    port_ret_ls4.append(port_ret4)\n",
    "    port_ret_ls5.append(port_ret5)\n",
    "    \n",
    "port_ret_arr1=np.array(port_ret_ls1)\n",
    "cum_arr1=np.cumprod(1+port_ret_arr1/100)\n",
    "port_ret_arr2=np.array(port_ret_ls2)\n",
    "cum_arr2=np.cumprod(1+port_ret_arr2/100)\n",
    "port_ret_arr3=np.array(port_ret_ls3)\n",
    "cum_arr3=np.cumprod(1+port_ret_arr3/100)\n",
    "port_ret_arr4=np.array(port_ret_ls4)\n",
    "cum_arr4=np.cumprod(1+port_ret_arr4/100)\n",
    "port_ret_arr5=np.array(port_ret_ls5)\n",
    "cum_arr5=np.cumprod(1+port_ret_arr5/100)\n",
    "\n",
    "#绝对收益图\n",
    "plt.plot(cum_arr1,color=\"red\",label=\"Raw\")\n",
    "plt.plot(cum_arr2,color=\"green\",label=\"Newey_West\")\n",
    "plt.plot(cum_arr3,color=\"black\",label=\"Bayes_Shrinkage\")\n",
    "plt.plot(cum_arr4,color=\"brown\",label=\"Bias_adjust\")\n",
    "plt.plot(cum_arr5,color=\"purple\",label=\"equal_weight\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plot_abso.jpg\",dpi=500)\n",
    "plt.show()\n",
    "\n",
    "#相对收益图\n",
    "relative_arr1=cum_arr1/cum_mkt_arr\n",
    "relative_arr2=cum_arr2/cum_mkt_arr\n",
    "relative_arr3=cum_arr3/cum_mkt_arr\n",
    "relative_arr4=cum_arr4/cum_mkt_arr\n",
    "relative_arr5=cum_arr5/cum_mkt_arr\n",
    "plt.plot(relative_arr1,color=\"red\",label=\"Raw\")\n",
    "plt.plot(relative_arr2,color=\"green\",label=\"Newey_West\")\n",
    "plt.plot(relative_arr3,color=\"black\",label=\"Bayes_Shrinkage\")\n",
    "plt.plot(relative_arr4,color=\"brown\",label=\"Bias_adjust\")\n",
    "plt.plot(relative_arr5,color=\"purple\",label=\"equal_weight\")\n",
    "plt.legend()\n",
    "plt.savefig(\"plot_rela.jpg\",dpi=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#组合优化指标对比表格\n",
    "\n",
    "def indicator_calcu(arr):\n",
    "    vola=np.std(arr)\n",
    "    cum_arr=np.cumprod(1+arr/100)\n",
    "    max_retract=(np.max(cum_arr)-np.min(cum_arr[np.argmax(cum_arr):]))/np.max(cum_arr)\n",
    "    sharpe=np.mean(arr)/np.std(arr)\n",
    "    \n",
    "    year_ret=(cum_arr[-1]-1)*100*36/len(cum_arr)\n",
    "    \n",
    "    return year_ret,vola,max_retract,sharpe\n",
    "\n",
    "df=pd.DataFrame(index=[\"Raw\",\"Newey_West\",\"Bayes_Shrinkage\",\"Bias_adjust\",\"equal_weight\"],columns=[\"yearly_ret\",\"vloa\",\"max_retract\",\"sharpe\"])\n",
    "df.iloc[0,:]=list(indicator_calcu(port_ret_arr1[:-1]))\n",
    "df.iloc[1,:]=list(indicator_calcu(port_ret_arr2[:-1]))\n",
    "df.iloc[2,:]=list(indicator_calcu(port_ret_arr3[:-1]))\n",
    "df.iloc[3,:]=list(indicator_calcu(port_ret_arr4[:-1]))\n",
    "df.iloc[4,:]=list(indicator_calcu(port_ret_arr5[:-1]))\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
